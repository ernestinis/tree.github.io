<!-- Copyright 2023 The MediaPipe Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License. -->
<!--<link href="https://unpkg.com/material-components-web@latest/dist/material-components-web.min.css" rel="stylesheet">-->
<!--<link href="hand.css" rel="stylesheet">-->
<style>
video {
  clear: both;
  display: block;
  transform: rotateY(180deg);
  -webkit-transform: rotateY(180deg);
  -moz-transform: rotateY(180deg);
  height: 280px;
}
.output_canvas {
  transform: rotateY(180deg);
  -webkit-transform: rotateY(180deg);
  -moz-transform: rotateY(180deg);
}
</style>

<script src="https://unpkg.com/material-components-web@latest/dist/material-components-web.min.js"></script>
<script  type="module"  >
import { GestureRecognizer, FilesetResolver, DrawingUtils } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3";
const demosSection = document.getElementById("demos");
let gestureRecognizer;
let runningMode = "VIDEO";
let enableWebcamButton;
let webcamRunning = false;
const videoHeight = "720px";//"360px";
const videoWidth = "960px";//"480px";
// Before we can use HandLandmarker class we must wait for it to finish
// loading. Machine Learning models can be large and take a moment to
// get everything needed to run.
const createGestureRecognizer = async () => {
    const vision = await FilesetResolver.forVisionTasks("https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm");
	
    gestureRecognizer = await GestureRecognizer.createFromOptions(vision, {
        baseOptions: {
            modelAssetPath: "https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task",
            delegate: "GPU"
        },
        runningMode: runningMode,
		numHands: 2
    });
    demosSection.classList.remove("invisible");
};
createGestureRecognizer();
/********************************************************************
// Demo 2: Continuously grab image from webcam stream and detect it.
********************************************************************/
const video = document.getElementById("webcam");
const canvasElement = document.getElementById("output_canvas");
const canvasCtx = canvasElement.getContext("2d");
const gestureOutput = document.getElementById("gesture_output");
// Check if webcam access is supported.
function hasGetUserMedia() {
    return !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia);
}
// If webcam supported, add event listener to button for when user
// wants to activate it.
if (hasGetUserMedia()) {
    enableWebcamButton = document.getElementById("webcamButton");
    enableWebcamButton.addEventListener("click", enableCam);
}
else {
    console.warn("getUserMedia() is not supported by your browser");
}
// Enable the live webcam view and start detection.
function enableCam(event) {
    if (!gestureRecognizer) {
        alert("Please wait for gestureRecognizer to load");
        return;
    }
    if (webcamRunning === true) {
        webcamRunning = false;
        enableWebcamButton.innerText = "ENABLE PREDICTIONS";
    }
    else {
        webcamRunning = true;
        enableWebcamButton.innerText = "DISABLE PREDICTIONS";
    }
    // getUsermedia parameters.
    const constraints = {
        video: true
    };
    // Activate the webcam stream.
    navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {
        video.srcObject = stream;
        video.addEventListener("loadeddata", predictWebcam);
    });
}
let lastVideoTime = -1;
let results = undefined;
let isGrab = false;
let ding = new Audio('ding.mp3');

var context = new AudioContext()
var o = context.createOscillator()
	o.type = "sine"
	o.connect(context.destination)

var grabStart = null;

async function predictWebcam() {
    const webcamElement = document.getElementById("webcam");
    // Now let's start detecting the stream.
    if (runningMode === "IMAGE") {
        runningMode = "VIDEO";
        await gestureRecognizer.setOptions({ runningMode: "VIDEO" });
    }
    let nowInMs = Date.now();
    if (video.currentTime !== lastVideoTime) {
        lastVideoTime = video.currentTime;
        results = gestureRecognizer.recognizeForVideo(video, nowInMs);
    }
    canvasCtx.save();
    canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
    const drawingUtils = new DrawingUtils(canvasCtx);
    canvasElement.style.height = videoHeight;
    webcamElement.style.height = videoHeight;
    canvasElement.style.width = videoWidth;
    webcamElement.style.width = videoWidth;
	
	var found = false;
	isGrab = false;
    if (results.landmarks) {
		
        for (const landmarks of results.landmarks) {
			if(true)
			{
				drawingUtils.drawConnectors(landmarks, GestureRecognizer.HAND_CONNECTIONS, {
					color: "#00FF00",
					lineWidth: 5
				});
				drawingUtils.drawLandmarks(landmarks, {
					color: "#FF0000",
					lineWidth: 2
				});
			}
			
		    var f1 = landmarks[8];
		    var f2 = landmarks[4];
			
			if(Math.abs(f1.x - f2.x)<0.05 && Math.abs(f1.y - f2.y)<0.05 && Math.abs(f1.z - f2.z)<0.05)
			{
				isGrab = true;
				canvasCtx.fillRect(canvasElement.width*f1.x-9,canvasElement.height*f1.y-9,18,18 );
				canvasCtx.fillRect(canvasElement.width*f2.x-9,canvasElement.height*f2.y-9,18,18 );
				if(grabStart==null)
				{
					grabStart = { x : (f1.x+f2.x)/2.0, y : (f1.y+f2.y)/2.0 }
					
					//new Audio('ding.mp3').play();
					//ding.play();
				}
				canvasCtx.beginPath();
				canvasCtx.moveTo(canvasElement.width*grabStart.x,canvasElement.height*grabStart.y,);
				canvasCtx.lineTo(canvasElement.width*f1.x,canvasElement.height*f1.y); 
				canvasCtx.stroke();
			}
			else{
				canvasCtx.fillRect(canvasElement.width*f1.x-4,canvasElement.height*f1.y-4,8,8 );
				canvasCtx.fillRect(canvasElement.width*f2.x-4,canvasElement.height*f2.y-4,8,8 );
			}
			
			//for( const landmark of landmarks ){
			//	canvasCtx.fillRect(canvasElement.width*landmark.x,canvasElement.height*landmark.y,8,8 );
			//}
			//canvasElement.style.position.left = (landmarks[0].x*100) + "px"  ;
        }
		
    }
	
	if(!isGrab)
	{
		grabStart = null;
	}
	//o.start()

	
    canvasCtx.restore();
    if (results.gestures.length > 0) {
        gestureOutput.style.display = "block";
        gestureOutput.style.width = videoWidth;
        const categoryName = results.gestures[0][0].categoryName;
        const categoryScore = parseFloat(results.gestures[0][0].score * 100).toFixed(2);
        const handedness = results.handednesses[0][0].displayName;
        gestureOutput.innerText = `GestureRecognizer: ${categoryName}\n Confidence: ${categoryScore} %\n Handedness: ${handedness}`;
    }
    else {
        gestureOutput.style.display = "none";
    }
    // Call this function again to keep predicting when the browser is ready.
    if (webcamRunning === true) {
        window.requestAnimationFrame(predictWebcam);
    }
}



</script>

<h1>Recognize hand gestures using the MediaPipe HandGestureRecognizer task</h1>

<section id="demos" class="invisible">
  <h2>Demo: Recognize gestures</h2>

  <div id="liveView" class="videoView">
    <button id="webcamButton" class="mdc-button mdc-button--raised">
      <span class="mdc-button__ripple"></span>
      <span class="mdc-button__label">ENABLE WEBCAM</span>
    </button>
    <div style="position: relative;">
      <video id="webcam" autoplay playsinline></video>
      <canvas class="output_canvas" id="output_canvas" width="960" height="720" style="position: absolute; left: 0px; top: 0px;"></canvas>
      <p id='gesture_output' class="output">
    </div>
  </div>
</section>